# prev_cost = Inf

@doc raw"""

```julia
function dynamics(xáµ¢::AbstractArray{T,1}, uáµ¢::AbstractArray{T,1})
    ...
    return xáµ¢â‚Šâ‚
end
```

```julia
function immediate_cost(x::AbstractArray{T,2}, u::AbstractArray{T,2})
    sum(u.^2)  # for example
end
```

```julia
function final_cost(xâ‚™::AbstractArray{T,1})
    sum((some_target_point - xâ‚™).^2)  # Euclidean distance at end, for example
end
```
"""
function forward_pass(x::AbstractMatrix{T}, u::AbstractMatrix{T},
                      ğ›¿ğ®á¶ á¶ s::AbstractMatrix{T}, ğŠs::AbstractArray{T,3},
                      prev_cost::T, dynamicsf::Function,
                      immediate_cost::Function, final_cost::Function
                      ) where {T}
    M, input_size = size(u); N, state_size = size(x)
    @assert(N == M+1)

    xÌ… = zeros(T, N, state_size); uÌ… = zeros(T, N-1, input_size)
    xÌ…[1, :] .= x[1, :]
    Î± = 1.0     # Learning rate
    total_cost = total_cost_generator(immediate_cost, final_cost)
    new_cost = 0.0

    while true
        for k = 1:N-1
            Î´xáµ¢ = xÌ…[k,:] - x[k,:]
            uÌ…[k,:] .= u[k,:] + Î± * ğ›¿ğ®á¶ á¶ s[k,:] + ğŠs[k,:,:] * Î´xáµ¢
            xÌ…[k+1,:] .= dynamicsf(xÌ…[k,:], uÌ…[k,:])
        end
        new_cost = total_cost(xÌ…, uÌ…)
        Î”cost = prev_cost - new_cost

        if (Î”cost > 0)    # Check if we should persue line search
            break
        else    # Catch overshoot as well as NaNs
            Î± /= 2
            display("Were line searchin'")
            display(Î±)
            display(any(isnan, uÌ…))
        end
    end

    @assert !any(isnan, uÌ…)
    @assert !any(isnan, xÌ…)

    return (xÌ…, uÌ…, new_cost)
end


@doc raw"""
fit usig iLQR
"""
function fit(x_init::AbstractMatrix{T}, u_init::AbstractMatrix{T},
             dynamicsf::Function, immediate_cost::Function,
             final_cost::Function; max_iter::Int64=100, tol::Float64=1e-6,
             ) where {T}
    xÌ…â± = x_init; uÌ…â± = u_init
    N, state_size = size(xÌ…â±); M, input_size = size(uÌ…â±)
    @assert(N == M + 1,
            "size(x_init)[2] == size(u_init)[1], (# of states is 1 more than # of inputs in trajectory)"
            )
    total_cost = total_cost_generator(immediate_cost, final_cost)

    prev_cost = Inf; new_cost = NaN
    iter = 0
    for iter = 1:max_iter
        ğ›¿ğ®á¶ á¶ s, ğŠs = backward_pass(xÌ…â±, uÌ…â±, dynamicsf, immediate_cost, final_cost)
        xÌ…â±âºÂ¹, uÌ…â±âºÂ¹, new_cost = forward_pass(xÌ…â±, uÌ…â±, ğ›¿ğ®á¶ á¶ s, ğŠs, prev_cost,
                                            dynamicsf, immediate_cost,
                                            final_cost)
        @assert(prev_cost > new_cost); prev_cost = new_cost

        # Check if we have met the tolerance for convergence
        display(size(uÌ…â±))
        display(size(uÌ…â±âºÂ¹))

        convert(Float64, sum((uÌ…â±âºÂ¹ - uÌ…â±).^2)) <= tol && break

        # Update the current trajectory and input estimates
        xÌ…â± = xÌ…â±âºÂ¹
        uÌ…â± = uÌ…â±âºÂ¹
    end

    return (xÌ…â±, uÌ…â±)
end


function total_cost_generator(immediate_cost::Function, final_cost::Function)
    function total_cost(xÌ…â±, uÌ…â±)
        N = size(uÌ…â±)[1]
        sum = 0.

        for i in 1:N
            sum += immediate_cost(xÌ…â±[i,:], uÌ…â±[i,:])
        end
        sum += final_cost(xÌ…â±[end,:])
    end

    return total_cost
end
