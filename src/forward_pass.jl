@doc raw"""
`forward_pass(x, u, ğ›¿ğ®á¶ á¶ s, ğŠs, prev_cost, dynamicsf, immediate_cost, final_cost)`

Perform iterativer LQR to compute optimal input and corresponding state
trajectory.

# Arguments
- `x::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `u::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `ğ›¿ğ®á¶ á¶ s::AbstractMatrix{T}`: see output of [`backward_pass(x, u, dynamicsf, immediate_cost, final_cost)`](@ref)
- `ğŠs::AbstractArray{T,3}`: see output of [`backward_pass(x, u, dynamicsf, immediate_cost, final_cost)`](@ref)
- `dynamicsf::Function`: dynamic function, steps the system forward
- `immediate_cost::Function`: Cost after each step
- `final_cost::Function`: Cost after final step

The `dynamicsf` steps the system forward (``x_{i+1} = f(x_i, u_i)``). The
function expects input of the form:
```julia
function dynamics(xáµ¢::AbstractVector{T}, uáµ¢::AbstractVector{T}) where T
    ...
    return xáµ¢â‚Šâ‚
end
```

The `immediate_cost` function expect input of the form:
```julia
function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
    return sum(u.^2) + sum(target_state - x.^2)  # for example
end
```
!!! note
    It is important that the function `immediate_cost` be an explict function
    of both `x` and `u` (due to issues using `ForwardDiff` Package). If you want
    to make `immediate_cost` practically only dependent on `u` with the following

    ```julia
    function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
        return sum(u.^2) + sum(x) * 0.0  # Only dependent on u
    end
    ```

The `final_cost` function expect input of the form:
```julia
function final_cost(x::AbstractVector{T})
    return sum(target_state - x.^2)  # for example
end
```

Returns the optimal trajectory ``(xÌ…, uÌ…)``
"""
function forward_pass(x::AbstractMatrix{T}, u::AbstractMatrix{T},
                      ğ›¿ğ®á¶ á¶ s::AbstractMatrix{T}, ğŠs::AbstractArray{T,3},
                      prev_cost::T, dynamicsf::Function,
                      immediate_cost::Function, final_cost::Function
                      ) where {T}
    M, input_size = size(u); N, state_size = size(x)
    @assert(N == M+1)

    xÌ… = zeros(T, N, state_size); uÌ… = zeros(T, N-1, input_size)
    xÌ…[1, :] .= x[1, :]
    Î± = 1.0     # Learning rate
    total_cost = total_cost_generator(immediate_cost, final_cost)
    new_cost = 0.0

    while true
        for k = 1:N-1
            Î´xáµ¢ = xÌ…[k,:] - x[k,:]
            uÌ…[k,:] .= u[k,:] + Î± * ğ›¿ğ®á¶ á¶ s[k,:] + ğŠs[k,:,:] * Î´xáµ¢
            xÌ…[k+1,:] .= dynamicsf(xÌ…[k,:], uÌ…[k,:])
        end
        new_cost = total_cost(xÌ…, uÌ…)
        Î”cost = prev_cost - new_cost

        if (Î”cost > 0)    # Check if we should persue line search
            break
        else    # Catch overshoot as well as NaNs
            Î± /= 2
            display("Were line searchin'")
            display(Î±)
            display(any(isnan, uÌ…))
        end
    end

    @assert !any(isnan, uÌ…)
    @assert !any(isnan, xÌ…)

    return (xÌ…, uÌ…, new_cost)
end


@doc raw"""
`fit(x_init, u_init, dynamicsf, immediate_cost, final_cost; max_iter=100, tol=1e-6)`

Perform iterativer LQR to compute optimal input and corresponding state
trajectory.

# Arguments
- `x_init::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `u_init::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `dynamicsf::Function`: dynamic function, steps the system forward
- `immediate_cost::Function`: Cost after each step
- `final_cost::Function`: Cost after final step
- `max_iter::Int64=100`: Maximum number of forward/backward passes to make
- `tol::Float64=1e-6`: Specifies the tolerance at which to consider the input
trajectory has converged

The `dynamicsf` steps the system forward (``x_{i+1} = f(x_i, u_i)``). The
function expects input of the form:
```julia
function dynamics(xáµ¢::AbstractVector{T}, uáµ¢::AbstractVector{T}) where T
    ...
    return xáµ¢â‚Šâ‚
end
```

The `immediate_cost` function expect input of the form:
```julia
function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
    return sum(u.^2) + sum(target_state - x.^2)  # for example
end
```
!!! note
    It is important that the function `immediate_cost` be an explict function
    of both `x` and `u` (due to issues using `ForwardDiff` Package). If you want
    to make `immediate_cost` practically only dependent on `u` with the following

    ```julia
    function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
        return sum(u.^2) + sum(x) * 0.0  # Only dependent on u
    end
    ```

The `final_cost` function expect input of the form:
```julia
function final_cost(x::AbstractVector{T})
    return sum(target_state - x.^2)  # for example
end
```

Returns the optimal trajectory ``(xÌ…, uÌ…)``
"""
function fit(x_init::AbstractMatrix{T}, u_init::AbstractMatrix{T},
             dynamicsf::Function, immediate_cost::Function,
             final_cost::Function; max_iter::Int64=100, tol::Float64=1e-6,
             ) where {T}
    xÌ…â± = x_init; uÌ…â± = u_init
    N, state_size = size(xÌ…â±); M, input_size = size(uÌ…â±)
    @assert(N == M + 1, "size(x_init)[2] == size(u_init)[1], (# of states is 1 more than # of inputs in trajectory)")
    total_cost = total_cost_generator(immediate_cost, final_cost)

    prev_cost = Inf; new_cost = NaN
    iter = 0
    for iter = 1:max_iter
        ğ›¿ğ®á¶ á¶ s, ğŠs = backward_pass(xÌ…â±, uÌ…â±, dynamicsf, immediate_cost, final_cost)
        xÌ…â±âºÂ¹, uÌ…â±âºÂ¹, new_cost = forward_pass(xÌ…â±, uÌ…â±, ğ›¿ğ®á¶ á¶ s, ğŠs, prev_cost,
                                            dynamicsf, immediate_cost,
                                            final_cost)
        @assert(prev_cost > new_cost); prev_cost = new_cost

        # Check if we have met the tolerance for convergence
        display(size(uÌ…â±))
        display(size(uÌ…â±âºÂ¹))

        convert(Float64, sum((uÌ…â±âºÂ¹ - uÌ…â±).^2)) <= tol && break

        # Update the current trajectory and input estimates
        xÌ…â± = xÌ…â±âºÂ¹
        uÌ…â± = uÌ…â±âºÂ¹
    end

    return (xÌ…â±, uÌ…â±)
end

@doc raw"""
`backward_pass(x, u, dynamicsf, immediate_cost, final_cost)`

Computes feedforward and feedback gains (``ğ›¿ğ®áµ¢á¶ á¶ `` and ``ğŠáµ¢``).

# Arguments
- `immediate_cost::Function`: Cost after each step
- `final_cost::Function`: Cost after final step

The `immediate_cost` function expect input of the form:
```julia
function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
    return sum(u.^2) + sum(target_state - x.^2)  # for example
end
```
!!! note
    It is important that the function `immediate_cost` be an explict function
    of both `x` and `u` (due to issues using `ForwardDiff` Package). If you want
    to make `immediate_cost` practically only dependent on `u` with the following

    ```julia
    function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
        return sum(u.^2) + sum(x) * 0.0  # Only dependent on u
    end
    ```

The `final_cost` function expect input of the form:
```julia
function final_cost(x::AbstractVector{T})
    return sum(target_state - x.^2)  # for example
end
```

Returns a function `total_cost` which computes the total cost of a state and
input trajectory.
"""
function total_cost_generator(immediate_cost::Function, final_cost::Function)
    function total_cost(xÌ…â±, uÌ…â±)
        N = size(uÌ…â±)[1]
        sum = 0.

        for i in 1:N
            sum += immediate_cost(xÌ…â±[i,:], uÌ…â±[i,:])
        end
        sum += final_cost(xÌ…â±[end,:])
    end

    return total_cost
end
