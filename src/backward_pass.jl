using ForwardDiff: gradient, jacobian, hessian
using LinearAlgebra: svd, Diagonal, inv, I

@doc raw"""
`linearize_dynamics(x, u, dynamicsf)`

linearizes the function `dynamicsf` around the point `x` and `u`.

# Arguments
- `x::AbstractVector{T}`: state at a specific step
- `u::AbstractVector{T}`: input at a specific step
- `dynamicsf::Function`: dynamic function, steps the system forward

The `dynamicsf` steps the system forward (``x_{i+1} = f(x_i, u_i)``). The
function expects input of the form:

```julia
function dynamics(xáµ¢::AbstractVector{T}, uáµ¢::AbstractVector{T}) where T
    ...
    return xáµ¢â‚Šâ‚
end
```

Returns ``(A, B)``, which are matricies defined below.

``f(x_k, u_k) â‰ˆ A x_k + B u_k``
"""
function linearize_dynamics(x::AbstractVector{T}, u::AbstractVector{T},
                            dynamicsf::Function) where {T}
    state_size = size(x)[1]; control_size = size(u)[1];

    ğ€ = zeros(T, state_size, state_size)
    ğ = zeros(T, state_size, control_size)

    # Declaring dynamics jacobian functions
    A_func(x, u) = jacobian(x -> dynamicsf(x, u), x)
    B_func(x, u) = jacobian(u -> dynamicsf(x, u), u)

    # Computing jacobian around each point
    ğ€ .= A_func(x, u)
    ğ .= B_func(x, u)

    return (ğ€, ğ)
end


@doc raw"""
`immediate_cost_quadratization(x, u, immediate_cost)`

Turns cost function into a quadratic at time step ``i`` around a point ``(xáµ¢, uáµ¢)``.
Details given in ETH slides.

# Arguments
- `x::AbstractVector{T}`: state at a specific step
- `u::AbstractVector{T}`: input at a specific step
- `immediate_cost::Function`: Cost after each step

The `immediate_cost` function expect input of the form:
```julia
function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
    return sum(u.^2) + sum(target_state - x.^2)  # for example
end
```

!!! note
    It is important that the function `immediate_cost` be an explict function
    of both `x` and `u` (due to issues using `ForwardDiff` Package). If you want
    to make `immediate_cost` practically only dependent on `u` use the following

    ```julia
    function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
        return sum(u.^2) + sum(x) * 0.0  # Only dependent on u
    end
    ```

Returns the matricies `(ğ‘áµ¢, ğªáµ¢, ğ«áµ¢, ğáµ¢, ğáµ¢, ğ‘áµ¢)` defined as:

``ğ‘áµ¢ = L(xáµ¢,uáµ¢)``, ``ğªáµ¢ = \frac{âˆ‚L(xáµ¢,uáµ¢)}{âˆ‚x}``, ``ğ«áµ¢ = \frac{âˆ‚L(xáµ¢,uáµ¢)}{âˆ‚u}``,
``ğáµ¢ = \frac{âˆ‚^2 L(xáµ¢,uáµ¢)}{âˆ‚x^2}``, ``ğáµ¢ = \frac{âˆ‚^2 L(xáµ¢,uáµ¢)}{âˆ‚x âˆ‚u}``,
``ğ‘áµ¢ = \frac{âˆ‚^2 L(xáµ¢,uáµ¢)}{âˆ‚u^2}``
"""
function immediate_cost_quadratization(x::AbstractVector{T},
                                       u::AbstractVector{T},
                                       immediate_cost::Function) where {T}
    state_size = size(x)[1]; control_size = size(u)[1];

    # Notation copied from ETH lecture notes
    ğ‘áµ¢ = convert(T, 0.)  # Cost along path
    ğªáµ¢ = zeros(T, state_size)  # Cost Jacobian wrt x
    ğ«áµ¢ = zeros(T, control_size)  # Cost Jacobian wrt u
    ğáµ¢ = zeros(T, state_size, state_size)  # Cost Hessian wrt x, x
    ğáµ¢ = zeros(T, control_size, state_size)  # Cost Hessian wrt u, x
    ğ‘áµ¢ = zeros(T, control_size, control_size)  # Cost Hessian wrt u, u

    # Helper jacobain functions
    âˆ‚Lâˆ‚x(x, u) = gradient(x -> immediate_cost(x, u), x)
    âˆ‚Lâˆ‚u(x, u) = gradient(u -> immediate_cost(x, u), u)
    âˆ‚Â²Lâˆ‚xÂ²(x, u) = hessian(x -> immediate_cost(x, u), x)
    âˆ‚Â²Lâˆ‚uâˆ‚x(x, u) = jacobian(x -> âˆ‚Lâˆ‚u(x, u), x)
    âˆ‚Â²Lâˆ‚uÂ²(x, u) = hessian(u -> immediate_cost(x, u), u)

    ğ‘áµ¢ = immediate_cost(x, u)
    ğªáµ¢ = âˆ‚Lâˆ‚x(x, u)        # Cost gradient wrt x
    ğ«áµ¢ = âˆ‚Lâˆ‚u(x, u)        # Cost gradient wrt u
    ğáµ¢ = âˆ‚Â²Lâˆ‚xÂ²(x, u)      # Cost Hessian wrt x, x
    ğáµ¢ = âˆ‚Â²Lâˆ‚uâˆ‚x(x, u)     # Cost Hessian wrt u, x
    ğ‘áµ¢ = âˆ‚Â²Lâˆ‚uÂ²(x, u)      # Cost Hessian wrt u, u

    return (ğ‘áµ¢, ğªáµ¢, ğ«áµ¢, ğáµ¢, ğáµ¢, ğ‘áµ¢)
end


@doc raw"""
`final_cost_quadratization(x, final_cost)`

Turns final cost function into a quadratic at last time step, `n`, about point
`(xâ‚™, uâ‚™)`. Details given in ETH slides.

# Arguments
- `x::AbstractVector{T}`: state at a specific step
- `final_cost::Function`: Cost after final step

The `final_cost` function expect input of the form:
```julia
function final_cost(x::AbstractVector{T})
    return sum(target_state - x.^2)  # for example
end
```

Returns the matricies `(ğ‘â‚™, ğªâ‚™, ğâ‚™)` defined as:

``ğ‘â‚™ = L(xâ‚™,uâ‚™)``, ``ğªâ‚™ = \frac{âˆ‚L(xâ‚™,uâ‚™)}{âˆ‚x}``, ``ğâ‚™ = \frac{âˆ‚^2 L(xâ‚™,uâ‚™)}{âˆ‚x^2}``
"""
function final_cost_quadratization(x::AbstractVector{T}, final_cost::Function) where {T}
    state_size = size(x)[1];

    # Notation copied from ETH lecture notes
    ğ‘â‚™ = convert(T, 0.)  # Cost along path
    ğªâ‚™ = zeros(T, state_size)  # Cost Jacobian wrt x
    ğâ‚™ = zeros(T, state_size, state_size)  # Cost Hessian wrt x, x

    # Helper jacobain functions
    âˆ‚Î¦âˆ‚x(x) = gradient(x -> final_cost(x), x)
    âˆ‚Â²Î¦âˆ‚xÂ²(x) = hessian(x -> final_cost(x), x)

    # Final cost
    ğ‘â‚™ = final_cost(x)
    # Final cost gradient wrt x
    ğªâ‚™ = âˆ‚Î¦âˆ‚x(x)
    # Final cost Hessian wrt x, x
    ğâ‚™ = âˆ‚Â²Î¦âˆ‚xÂ²(x)

    return (ğ‘â‚™, ğªâ‚™, ğâ‚™)
end


@doc raw"""
`optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`

Computes optimal control parameters `(ğ áµ¢, ğ†áµ¢, ğ‡áµ¢)`, at time step `i`.
These are used in computing feedforward and feedback gains.

# Arguments
- `ğ€áµ¢::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `ğáµ¢::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `ğ«áµ¢::AbstractVector{T}`: see output of [`immediate_cost_quadratization(x, u, immediate_cost)`](@ref)
- `ğáµ¢::AbstractMatrix{T}`: see output of [`immediate_cost_quadratization(x, u, immediate_cost)`](@ref)
- `ğ‘áµ¢::AbstractMatrix{T}`: see output of [`immediate_cost_quadratization(x, u, immediate_cost)`](@ref)
- `ğ¬áµ¢â‚Šâ‚::AbstractVector{T}`: Rollback parameter
- `ğ’áµ¢â‚Šâ‚::AbstractMatrix{T}`: Rollback parameter

Returns the matricies `(ğ áµ¢, ğ†áµ¢, ğ‡áµ¢)` defined as:

``ğ áµ¢ = ğ«áµ¢ + ğáµ¢^T ğ¬áµ¢â‚Šâ‚``, ``ğ†áµ¢ = ğáµ¢ + ğáµ¢^T ğ’áµ¢â‚Šâ‚ ğ€áµ¢``,
``ğ‡áµ¢ = ğ‘áµ¢ + ğáµ¢^T ğ’áµ¢â‚Šâ‚ ğáµ¢``
"""
function optimal_controller_param(ğ€áµ¢::AbstractMatrix{T}, ğáµ¢::AbstractMatrix{T},
                                  ğ«áµ¢::AbstractVector{T}, ğáµ¢::AbstractMatrix{T},
                                  ğ‘áµ¢::AbstractMatrix{T}, ğ¬áµ¢â‚Šâ‚::AbstractVector{T},
                                  ğ’áµ¢â‚Šâ‚::AbstractMatrix{T}) where {T}
    ğ áµ¢ = ğ«áµ¢ + ğáµ¢' * ğ¬áµ¢â‚Šâ‚
    ğ†áµ¢ = ğáµ¢ + ğáµ¢' * ğ’áµ¢â‚Šâ‚ * ğ€áµ¢
    ğ‡áµ¢ = ğ‘áµ¢ + ğáµ¢' * ğ’áµ¢â‚Šâ‚ * ğáµ¢

    return (ğ áµ¢, ğ†áµ¢, ğ‡áµ¢)
end


@doc raw"""
`feedback_parameters(ğ áµ¢, ğ†áµ¢, ğ‡áµ¢)`

Computes feedforward and feedback gains, ``(ğ›¿ğ®áµ¢á¶ á¶ , ğŠáµ¢)``.

# Arguments
- `ğ áµ¢::AbstractVector{T}`: see output of [`optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`](@ref)
- `ğ†áµ¢::AbstractMatrix{T}`: see output of [`optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`](@ref)
- `ğ‡áµ¢::AbstractMatrix{T}`: see output of [`optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`](@ref)

Returns the matricies `(ğ›¿ğ®áµ¢á¶ á¶ , ğŠáµ¢)` defined as:

``ğ›¿ğ®áµ¢á¶ á¶  = - ğ‡áµ¢^{-1} ğ áµ¢``, ``ğŠáµ¢ = - ğ‡áµ¢^{-1} ğ†áµ¢``

Because ``ğ‡áµ¢`` can be poorly conditioned, the regularized inverse of the matrix
is computed instead of the true inverse.
"""
function feedback_parameters(ğ áµ¢::AbstractVector{T}, ğ†áµ¢::AbstractMatrix{T},
                             ğ‡áµ¢::AbstractMatrix{T}) where {T}
    # ğ›¿ğ®áµ¢á¶ á¶  = - ğ‡áµ¢ \ ğ áµ¢
    # ğŠáµ¢ = - ğ‡áµ¢ \ ğ†áµ¢
    # H_inv = regularized_persudo_inverse(ğ‡áµ¢)

    n = size(ğ‡áµ¢)[1]
    H_inv = inv(ğ‡áµ¢ + 0.01 * I(n))
    ğ›¿ğ®áµ¢á¶ á¶  = - H_inv * ğ áµ¢
    ğŠáµ¢ = - H_inv * ğ†áµ¢
    return (ğ›¿ğ®áµ¢á¶ á¶ , ğŠáµ¢)
end


function regularized_persudo_inverse(matrix::AbstractMatrix{T}; reg=1e-5) where {T}
    @assert !any(isnan, matrix)

    SVD = svd(matrix)

    SVD.S[ SVD.S .< 0 ] .= 0.0        #truncate negative values...
    diag_s_inv = zeros(T, (size(SVD.V)[1], size(SVD.U)[2]))
    diag_s_inv[1:length(SVD.S), 1:length(SVD.S)] .= Diagonal(1.0 / (SVD.S .+ reg))

    regularized_matrix = SVD.V * diag_s_inv * transpose(SVD.U)
    # println(regularized_matrix)
    return regularized_matrix
end


@doc raw"""
`step_back(ğ€áµ¢, ğ‘áµ¢, ğªáµ¢, ğáµ¢, ğ áµ¢, ğ†áµ¢, ğ‡áµ¢, ğ›¿ğ®áµ¢á¶ á¶ , ğŠáµ¢, ğ‘ áµ¢â‚Šâ‚, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`

Computes the rollback parameters ``ğ‘ áµ¢``, ``ğ¬áµ¢``, and ``ğ’áµ¢`` for the next step
backward.

# Arguments
- `ğ€áµ¢::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `ğáµ¢::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `ğ‘áµ¢::T`: see output of [`immediate_cost_quadratization(x, u, immediate_cost)`](@ref)
- `ğªáµ¢::AbstractVector{T}`: see output of [`immediate_cost_quadratization(x, u, immediate_cost)`](@ref)
- `ğáµ¢::AbstractMatrix{T}`: see output of [`immediate_cost_quadratization(x, u, immediate_cost)`](@ref)
- `ğ áµ¢::AbstractVector{T}`: see output of [`optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`](@ref)
- `ğ†áµ¢::AbstractMatrix{T}`: see output of [`optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`](@ref)
- `ğ‡áµ¢::AbstractMatrix{T}`: see output of [`optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)`](@ref)
- `ğ›¿ğ®áµ¢á¶ á¶ ::AbstractVector{T}`: see output of [`feedback_parameters(ğ áµ¢, ğ†áµ¢, ğ‡áµ¢)`](@ref)
- `ğŠáµ¢::AbstractMatrix{T}`: see output of [`feedback_parameters(ğ áµ¢, ğ†áµ¢, ğ‡áµ¢)`](@ref)
- `ğ‘ áµ¢â‚Šâ‚::T`: Rollback parameter
- `ğ¬áµ¢â‚Šâ‚::AbstractVector{T}`: Rollback parameter
- `ğ’áµ¢â‚Šâ‚::AbstractMatrix{T}`: Rollback parameter

Returns the next-step-back's rollback parameters, ``(ğ‘ áµ¢, ğ¬áµ¢, ğ’áµ¢)``

Because ``ğ‡áµ¢`` can be poorly conditioned, the regularized inverse of the matrix
is computed instead of the true inverse.
"""
function step_back(ğ€áµ¢::AbstractMatrix{T}, ğ‘áµ¢::T, ğªáµ¢::AbstractVector{T},
                   ğáµ¢::AbstractMatrix{T}, ğ áµ¢::AbstractVector{T},
                   ğ†áµ¢::AbstractMatrix{T}, ğ‡áµ¢::AbstractMatrix{T},
                   ğ›¿ğ®áµ¢á¶ á¶ ::AbstractVector{T}, ğŠáµ¢::AbstractMatrix{T},
                   ğ‘ áµ¢â‚Šâ‚::T, ğ¬áµ¢â‚Šâ‚::AbstractVector{T}, ğ’áµ¢â‚Šâ‚::AbstractMatrix{T}
                   ) where {T}
    ğ‘ áµ¢ = (ğ‘áµ¢ + ğ‘ áµ¢â‚Šâ‚ + .5 * ğ›¿ğ®áµ¢á¶ á¶ ' * ğ‡áµ¢ * ğ›¿ğ®áµ¢á¶ á¶  + ğ›¿ğ®áµ¢á¶ á¶ ' * ğ áµ¢)
    ğ¬áµ¢ = (ğªáµ¢ + ğ€áµ¢' * ğ¬áµ¢â‚Šâ‚ + ğŠáµ¢' * ğ‡áµ¢ * ğ›¿ğ®áµ¢á¶ á¶  + ğŠáµ¢' * ğ áµ¢ + ğ†áµ¢' * ğ›¿ğ®áµ¢á¶ á¶ )
    ğ’áµ¢ = (ğáµ¢ + ğ€áµ¢' * ğ’áµ¢â‚Šâ‚ * ğ€áµ¢ + ğŠáµ¢' * ğ‡áµ¢ * ğŠáµ¢ + ğŠáµ¢' * ğ†áµ¢ + ğ†áµ¢' * ğŠáµ¢)

    return (ğ‘ áµ¢, ğ¬áµ¢, ğ’áµ¢)
end


@doc raw"""
`backward_pass(x, u, dynamicsf, immediate_cost, final_cost)`

Computes feedforward and feedback gains (``ğ›¿ğ®áµ¢á¶ á¶ `` and ``ğŠáµ¢``).

# Arguments
- `x::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `u::AbstractMatrix{T}`: see output of [`linearize_dynamics(x, u, dynamicsf)`](@ref)
- `dynamicsf::Function`: dynamic function, steps the system forward
- `immediate_cost::Function`: Cost after each step
- `final_cost::Function`: Cost after final step

The `dynamicsf` steps the system forward (``x_{i+1} = f(x_i, u_i)``). The
function expects input of the form:
```julia
function dynamics(xáµ¢::AbstractVector{T}, uáµ¢::AbstractVector{T}) where T
    ...
    return xáµ¢â‚Šâ‚
end
```

The `immediate_cost` function expect input of the form:
```julia
function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
    return sum(u.^2) + sum(target_state - x.^2)  # for example
end
```
!!! note
    It is important that the function `immediate_cost` be an explict function
    of both `x` and `u` (due to issues using `ForwardDiff` Package). If you want
    to make `immediate_cost` practically only dependent on `u` with the following

    ```julia
    function immediate_cost(x::AbstractVector{T}, u::AbstractVector{T})
        return sum(u.^2) + sum(x) * 0.0  # Only dependent on u
    end
    ```

The `final_cost` function expect input of the form:
```julia
function final_cost(x::AbstractVector{T})
    return sum(target_state - x.^2)  # for example
end
```

Returns the feedback parameters ``ğ›¿ğ®áµ¢á¶ á¶ áµ¢``, and ``ğŠáµ¢`` for each time step ``i``
"""
function backward_pass(x::AbstractMatrix{T}, u::AbstractMatrix{T},
                       dynamicsf::Function, immediate_cost::Function,
                       final_cost::Function) where {T}
    # Grab all dimensions
    N, state_size = size(x); M, input_size = size(u);
    @assert(N == M+1)

    # Initialize matricies
    ğ›¿ğ®á¶ á¶ s = zeros(T, N-1, input_size)
    ğŠs = zeros(T, N-1, input_size, state_size)

    (ğ‘â‚™, ğªâ‚™, ğâ‚™) = final_cost_quadratization(x[N,:], final_cost)
    (ğ‘ áµ¢â‚Šâ‚, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚) = (ğ‘â‚™, ğªâ‚™, ğâ‚™)

    # Move backward
    for i = (N-1):-1:1
        (ğ€áµ¢, ğáµ¢) = linearize_dynamics(x[i,:], u[i,:], dynamicsf)
        (ğ‘áµ¢, ğªáµ¢, ğ«áµ¢, ğáµ¢, ğáµ¢, ğ‘áµ¢) = immediate_cost_quadratization(x[i,:], u[i,:], immediate_cost)
        (ğ áµ¢, ğ†áµ¢, ğ‡áµ¢) = optimal_controller_param(ğ€áµ¢, ğáµ¢, ğ«áµ¢, ğáµ¢, ğ‘áµ¢, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)
        (ğ›¿ğ®áµ¢á¶ á¶ , ğŠáµ¢) = feedback_parameters(ğ áµ¢, ğ†áµ¢, ğ‡áµ¢)

        ğ›¿ğ®á¶ á¶ s[i,:] .= ğ›¿ğ®áµ¢á¶ á¶ 
        ğŠs[i,:,:] .= ğŠáµ¢

        (ğ‘ áµ¢, ğ¬áµ¢, ğ’áµ¢) = step_back(ğ€áµ¢, ğ‘áµ¢, ğªáµ¢, ğáµ¢, ğ áµ¢, ğ†áµ¢, ğ‡áµ¢, ğ›¿ğ®áµ¢á¶ á¶ , ğŠáµ¢,
                                ğ‘ áµ¢â‚Šâ‚, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚)
        (ğ‘ áµ¢â‚Šâ‚, ğ¬áµ¢â‚Šâ‚, ğ’áµ¢â‚Šâ‚) = (ğ‘ áµ¢, ğ¬áµ¢, ğ’áµ¢)
    end

    @assert !any(isnan, ğ›¿ğ®á¶ á¶ s)
    @assert !any(isnan, ğŠs)

    return (ğ›¿ğ®á¶ á¶ s, ğŠs)
end
